# may not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]
# Create training and test dataframes
train_data <- data.frame(
Q49 = y_train,
X_train[, selected_var_names]
)
test_data <- data.frame(
Q49 = y_test,
X_test[, selected_var_names]
)
# Model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)
# Model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)
# Quick look at model summaries
summary(lm)
summary(po)
# may not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]
# Create training and test dataframes
train_data <- data.frame(
Q49 = y_train,
X_train[, selected_var_names]
)
test_data <- data.frame(
Q49 = y_test,
X_test[, selected_var_names]
)
# Model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)
# Model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)
# Summarize (the output is so long)
# summary(lm)
# summary(po)
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(model_lm)
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
# Check VIF for multicollinearity
library(car)
vif(lm)
# Predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# Performance metrics
mae_lm <- mean(abs(lm - test_data$Q49))
# Predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# Performance metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# Compare
data.frame(
Model = c("Linear", "Proportional Odds"),
Test_MAE = c(mae_lm, mae_po),
Test_RMSE = c(rmse_lm, rmse_po)
)
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
# Predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# Performance metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# Compare
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean Absolute Error = c(mae_lm, mae_po),
# Predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# Performance metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# Compare
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po),
Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)
# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])
#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")
coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"
selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars
# he also suggested prediction intervals (maybe meant if we went in the coefficient estimation direction?)
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]
# create training and test dataframes
train_data <- data.frame(
Q49 = y_train,
X_train[, selected_var_names]
)
test_data <- data.frame(
Q49 = y_test,
X_test[, selected_var_names]
)
# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)
# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)
#summarize optional, output is very long
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# Performance metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# Compare
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po),
Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# metrics
rounded_mae_lm <- mean(abs(round(pred_lm) - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rounded_rmse_lm <- sqrt(mean((round(pred_lm) - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(rounded_mae_lm, mae_po),
Root_Mean_Square_Error = c(rounded_rmse_lm, rmse_po)
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# metrics
rounded_mae_lm <- mean(abs(round(pred_lm) - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rounded_rmse_lm <- sqrt(mean((round(pred_lm) - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(rounded_mae_lm, mae_po),
Root_Mean_Square_Error = c(rounded_rmse_lm, rmse_po)
)
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
dim(pred_po_probs)
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]
# create training and test dataframes
train_data <- data.frame(
Q49 = y_train,
X_train[, selected_var_names]
)
test_data <- data.frame(
Q49 = y_test,
X_test[, selected_var_names]
)
# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)
# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)
#summarize optional, output is very long
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
dim(pred_po_probs)
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)
# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
#wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)
# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])
#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")
coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"
selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars
# he also suggested prediction intervals (maybe meant if we went in the coefficient estimation direction?)
unique(df$column)
unqiue(wvs_clean$Q49)
unique(wvs_clean$Q49)
wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)
unique(wvs_clean$Q49)
# libraries/packages (ADD MORE depending on how we do rest of analysis)
library(readr)
library(MASS)
library(glmnet)
library(dplyr)
# WVS data
wvs <- read_csv("Resources/WVS_Cross-National_Wave_7_csv_v6_0.csv")
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)
# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)
# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])
#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")
coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"
selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars
# he also suggested prediction intervals (maybe meant if we went in the coefficient estimation direction?)
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]
# create training and test dataframes
train_data <- data.frame(
Q49 = y_train,
X_train[, selected_var_names]
)
test_data <- data.frame(
Q49 = y_test,
X_test[, selected_var_names]
)
# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)
# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)
#summarize optional, output is very long
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po),
Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
dim(pred_po_probs)
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# Compare them
head(data.frame(
Mode = pred_po_mode_int,
Expected = pred_po_expected,
Difference = pred_po_expected - pred_po_mode_int
))
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# Compare them
head(data.frame(
Mode = pred_po_mode_int,
Expected = pred_po_expected,
Difference = pred_po_expected - pred_po_mode_int
))
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# Compare them
head(data.frame(
Mode = pred_po_mode_int,
Expected = pred_po_expected,
Difference = pred_po_expected - pred_po_mode_int
))
mae_po_probs <- mean(abs(pred_po_probs - test_data$Q49))
rmse_po_probs <- sqrt(mean((pred_po_probs - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po_probs),
Root_Mean_Square_Error = c(rmse_lm, rmse_po_probs)
)
pred_po_mode <- predict(po, newdata = test_data)
pred_po_mode_int <- as.numeric(as.character(pred_po_mode))
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
# Compare them
head(data.frame(
Mode = pred_po_mode_int,
Expected = pred_po_expected,
Difference = pred_po_expected - pred_po_mode_int
))
mae_po_expected <- mean(abs(pred_po_expected - test_data$Q49))
rmse_po_expected <- sqrt(mean((pred_po_expected - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po_expected),
Root_Mean_Square_Error = c(rmse_lm, rmse_po_expected)
)
# try using expected value (continuous) instead of mode
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)
mae_po_expected <- mean(abs(pred_po_expected - test_data$Q49))
rmse_po_expected <- sqrt(mean((pred_po_expected - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po_expected),
Root_Mean_Square_Error = c(rmse_lm, rmse_po_expected)
)
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))
# metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))
# display comparison
data.frame(
Model = c("Linear", "Proportional Odds"),
Mean_Absolute_Error = c(mae_lm, mae_po),
Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
table(train_data$Q49)
hist(train_data$Q49)
library(brant)
install.packages("brant")
library(brant)
brant(po)
table(wvs_clean$Q49)
hist(wvs_clean$Q49)
library(nnet)
model_multinom <- multinom(factor(Q49) ~ ., data = train_data, maxit = 500)
library(glmnet)
library(dplyr)
rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))
mae  <- function(y, yhat) mean(abs(y - yhat))
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test  <- X[-train_indices, ]
y_test  <- y[-train_indices]
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
pred_1se <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.1se"))
pred_min <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.min"))
lambda_perf <- data.frame(
lambda_rule = c("lambda.1se", "lambda.min"),
RMSE = c(rmse(y_test, pred_1se), rmse(y_test, pred_min)),
MAE  = c(mae(y_test, pred_1se),  mae(y_test, pred_min))
)
coef_1se <- coef(cv_lasso, s = "lambda.1se")
coef_min <- coef(cv_lasso, s = "lambda.min")
vars_1se <- setdiff(rownames(coef_1se)[as.vector(coef_1se != 0)], "(Intercept)")
vars_min <- setdiff(rownames(coef_min)[as.vector(coef_min != 0)], "(Intercept)")
jaccard_overlap <- length(intersect(vars_1se, vars_min)) / length(union(vars_1se, vars_min))
print(lambda_perf)
length(vars_1se)
length(vars_min)
jaccard_overlap
seeds <- 1:30
split_df <- bind_rows(lapply(seeds, function(s) {
set.seed(s)
n <- nrow(X)
tr <- sample(1:n, size = round(0.7 * n))
X_tr <- X[tr, ]; y_tr <- y[tr]
X_te <- X[-tr, ]; y_te <- y[-tr]
cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1
data.frame(
seed = s,
RMSE = rmse(y_te, yhat),
MAE  = mae(y_te, yhat),
n_selected = nsel
)
}))
split_df %>%
summarize(
RMSE_mean = mean(RMSE), RMSE_sd = sd(RMSE),
MAE_mean  = mean(MAE),  MAE_sd  = sd(MAE),
nsel_mean = mean(n_selected), nsel_sd = sd(n_selected)
)
sensitive_vars <- c("Q260","Q262","Q273","Q240","Q150")
sens_in_data <- intersect(sensitive_vars, colnames(X))
run_lasso_once <- function(Xmat, yvec, seed = 123) {
set.seed(seed)
n <- nrow(Xmat)
tr <- sample(1:n, size = round(0.7 * n))
X_tr <- Xmat[tr, ]; y_tr <- yvec[tr]
X_te <- Xmat[-tr, ]; y_te <- yvec[-tr]
cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1
data.frame(
RMSE = rmse(y_te, yhat),
MAE  = mae(y_te, yhat),
n_selected = nsel
)
}
full_perf <- run_lasso_once(X, y)
if (length(sens_in_data) > 0) {
X_nosens <- X[, setdiff(colnames(X), sens_in_data), drop = FALSE]
nosens_perf <- run_lasso_once(X_nosens, y)
rbind(
data.frame(model = "All variables", full_perf),
data.frame(model = "Exclude sensitive vars", nosens_perf)
)
} else {
data.frame(model = "All variables", full_perf)
}
# use privacy and interest to conduct sensitivity analyses / check robustness of results
k <- 10
folds <- sample(1:k, nrow(train_data), replace = TRUE)
cv_errors_lm <- numeric(k)
cv_errors_po <- numeric(k)
for(i in 1:k) {
# split
cv_train <- train_data[folds != i, ]
cv_valid <- train_data[folds == i, ]
# Fit
fit_lm <- lm(Q49 ~ ., data = cv_train)
fit_po <- polr(factor(Q49) ~ ., data = cv_train)
# predict
pred_lm <- predict(fit_lm, newdata = cv_valid)
pred_po_probs <- predict(fit_po, newdata = cv_valid, type = "probs")
pred_po <- as.numeric(pred_po_probs %*% 1:10)
# calculate errors
cv_errors_lm[i] <- mean((pred_lm - cv_valid$Q49)^2)
cv_errors_po[i] <- mean((pred_po - cv_valid$Q49)^2)
}
# Results
cv_rmse_lm <- sqrt(mean(cv_errors_lm))
cv_rmse_po <- sqrt(mean(cv_errors_po))
# Compare
data.frame(
Model = c("Linear", "Proportional Odds"),
Test_RMSE = c(1.642, 1.649),
CV_RMSE = c(cv_rmse_lm, cv_rmse_po)
)
