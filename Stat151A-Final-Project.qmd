---
title: "Stat151A-Project-Analysis"
format: html
editor: visual
---

### Loading Data + Packages

```{r}
# libraries/packages (ADD MORE depending on how we do rest of analysis)
library(readr)
library(MASS)
library(glmnet)
library(dplyr)

# WVS data
wvs <- read_csv("Resources/WVS_Cross-National_Wave_7_csv_v6_0.csv")
```

### LASSO Variable Selection

```{r}
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)

# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)

# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])

#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]

# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")

coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"

selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars

# he also suggested prediction intervals (maybe meant if we went in the coefficient estimation direction?)
```

### Quick Check on Life Satisfaction

```{r}
table(wvs_clean$Q49)
hist(wvs_clean$Q49)
```

### Fitting Models

```{r}
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]

# create training and test dataframes
train_data <- data.frame(
  Q49 = y_train,
  X_train[, selected_var_names]
)

test_data <- data.frame(
  Q49 = y_test,
  X_test[, selected_var_names]
)

# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)

# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)

#summarize optional, output is very long
```

### Diagnostics

```{r}
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
```

### Evaluate on Test Set

```{r}
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))

# metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))

rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Linear", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_lm, mae_po),
  Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
```

Hunch that the intermediate values from lm is giving it better metrics than the proportional odds model which is forced to round to an integer.

### Evaluate on Test Set (pt 2)

```{r}
# try using expected value (continuous) instead of mode
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)

mae_po_expected <- mean(abs(pred_po_expected - test_data$Q49))
rmse_po_expected <- sqrt(mean((pred_po_expected - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Linear", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_lm, mae_po_expected),
  Root_Mean_Square_Error = c(rmse_lm, rmse_po_expected)
)
```

This is better but still bad, let's check proportional odds assumptions with brant test. Warning: this takes forever to run.

```{r}
library(brant)
brant(po)
```

Findings: assumptions hold for neither model

### Fitting Multinomial Logit

```{r}
library(nnet)
model_multinom <- multinom(factor(Q49) ~ ., data = train_data, maxit = 500)
```

### Sensitivity Analyses

```{r}
# use privacy and interest to conduct sensitivity analyses / check robustness of results
```
