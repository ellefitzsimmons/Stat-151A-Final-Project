---
title: "Stat151A-Project-Analysis"
format: html
editor: visual
---

### Loading Data + Packages

```{r}
# libraries/packages (ADD MORE depending on how we do rest of analysis)
library(readr)
library(MASS)
library(glmnet)
library(dplyr)

# WVS data
wvs <- read_csv("Resources/WVS_Cross-National_Wave_7_csv_v6_0.csv")
```

### LASSO Variable Selection

```{r}
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)

# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)

# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])

#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]

# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")

coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"

selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars

# he also suggested prediction intervals (maybe meant if we went in the coefficient estimation direction?)
```

### Quick Check on Life Satisfaction

```{r}
table(wvs_clean$Q49)
hist(wvs_clean$Q49)
```

### Fitting Models

```{r}
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]

# create training and test dataframes
train_data <- data.frame(
  Q49 = y_train,
  X_train[, selected_var_names]
)

test_data <- data.frame(
  Q49 = y_test,
  X_test[, selected_var_names]
)

# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)

# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)

#summarize optional, output is very long
```

### Diagnostics

```{r}
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
```

### Evaluate on Test Set

```{r}
# predictions
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))

# metrics
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))

rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Linear", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_lm, mae_po),
  Root_Mean_Square_Error = c(rmse_lm, rmse_po)
)
```

Hunch that the intermediate values from lm is giving it better metrics than the proportional odds model which is forced to round to an integer.

### Evaluate on Test Set (pt 2)

```{r}
# try using expected value (continuous) instead of mode
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)

mae_po_expected <- mean(abs(pred_po_expected - test_data$Q49))
rmse_po_expected <- sqrt(mean((pred_po_expected - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Linear", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_lm, mae_po_expected),
  Root_Mean_Square_Error = c(rmse_lm, rmse_po_expected)
)
```

This is better but still bad, let's check proportional odds assumptions with brant test. Warning: this takes forever to run.

```{r}
library(brant)
brant(po)
```

Findings: assumptions hold for neither model

### Fitting Multinomial Logit

```{r}
library(nnet)
model_multinom <- multinom(factor(Q49) ~ ., data = train_data, maxit = 500)
```

### Sensitivity Analyses

```{r}
library(glmnet)
library(dplyr)

rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))
mae  <- function(y, yhat) mean(abs(y - yhat))

set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test  <- X[-train_indices, ]
y_test  <- y[-train_indices]

cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)

pred_1se <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.1se"))
pred_min <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.min"))

lambda_perf <- data.frame(
  lambda_rule = c("lambda.1se", "lambda.min"),
  RMSE = c(rmse(y_test, pred_1se), rmse(y_test, pred_min)),
  MAE  = c(mae(y_test, pred_1se),  mae(y_test, pred_min))
)

coef_1se <- coef(cv_lasso, s = "lambda.1se")
coef_min <- coef(cv_lasso, s = "lambda.min")

vars_1se <- setdiff(rownames(coef_1se)[as.vector(coef_1se != 0)], "(Intercept)")
vars_min <- setdiff(rownames(coef_min)[as.vector(coef_min != 0)], "(Intercept)")

jaccard_overlap <- length(intersect(vars_1se, vars_min)) / length(union(vars_1se, vars_min))

print(lambda_perf)
length(vars_1se)
length(vars_min)
jaccard_overlap

seeds <- 1:30
split_df <- bind_rows(lapply(seeds, function(s) {
  set.seed(s)
  n <- nrow(X)
  tr <- sample(1:n, size = round(0.7 * n))
  X_tr <- X[tr, ]; y_tr <- y[tr]
  X_te <- X[-tr, ]; y_te <- y[-tr]

  cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
  yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
  nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1

  data.frame(
    seed = s,
    RMSE = rmse(y_te, yhat),
    MAE  = mae(y_te, yhat),
    n_selected = nsel
  )
}))

split_df %>%
  summarize(
    RMSE_mean = mean(RMSE), RMSE_sd = sd(RMSE),
    MAE_mean  = mean(MAE),  MAE_sd  = sd(MAE),
    nsel_mean = mean(n_selected), nsel_sd = sd(n_selected)
  )

sensitive_vars <- c("Q260","Q262","Q273","Q240","Q150")
sens_in_data <- intersect(sensitive_vars, colnames(X))

run_lasso_once <- function(Xmat, yvec, seed = 123) {
  set.seed(seed)
  n <- nrow(Xmat)
  tr <- sample(1:n, size = round(0.7 * n))
  X_tr <- Xmat[tr, ]; y_tr <- yvec[tr]
  X_te <- Xmat[-tr, ]; y_te <- yvec[-tr]

  cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
  yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
  nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1

  data.frame(
    RMSE = rmse(y_te, yhat),
    MAE  = mae(y_te, yhat),
    n_selected = nsel
  )
}

full_perf <- run_lasso_once(X, y)

if (length(sens_in_data) > 0) {
  X_nosens <- X[, setdiff(colnames(X), sens_in_data), drop = FALSE]
  nosens_perf <- run_lasso_once(X_nosens, y)
  rbind(
    data.frame(model = "All variables", full_perf),
    data.frame(model = "Exclude sensitive vars", nosens_perf)
  )
} else {
  data.frame(model = "All variables", full_perf)
}

# use privacy and interest to conduct sensitivity analyses / check robustness of results
```
