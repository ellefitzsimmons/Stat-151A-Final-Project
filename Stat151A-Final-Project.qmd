---
title: "Stat151A-Project-Analysis"
format: html
editor: visual
---

### Loading Data + Packages

```{r}
# libraries/packages (ADD MORE depending on how we do rest of analysis)
library(readr)
library(MASS)
library(glmnet)
library(dplyr)

# WVS data
wvs <- read_csv("Resources/WVS_Cross-National_Wave_7_csv_v6_0.csv")
```

### Clean Data & Subset Variables

```{r}
# variables
life_satisfaction = wvs["Q49"]
questions <- wvs %>% select(starts_with("Q"), -Q49)

# remove NA values
wvs_clean <- na.omit(cbind(life_satisfaction, questions))
wvs_clean <- wvs_clean %>% filter(Q49 >= 1 & Q49 <= 10)

# X as matrix, Y as vector
y <- wvs_clean$Q49
X <- as.matrix(wvs_clean[, -1])

#split training and test data
set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
```

### Original Model (linear, manual variable selection)

```{r}
# manually selected variables
original_predictors = c("Q50","Q52","Q71","Q94","Q95","Q96","Q101", 
                  "Q112","Q131","Q142","Q143","Q146","Q148","Q164",
                  "Q166","Q167","Q182","Q240","Q249","Q251","Q260",
                  "Q262","Q263","Q264","Q265","Q269","Q270","Q271",
                  "Q273","Q274","Q275")

# training data frame w/ selected variables
train_data <- wvs_clean[train_indices, ]

original_lm_data <- train_data[, c("Q49", original_predictors)]

# fit linear model
original_lm <- lm(Q49 ~ ., data = original_lm_data)
```

outlined in proposal, selected variables based on relevance, removing probable highly-correlated pairs

### LASSO Variable Selection

```{r}
# LASSO
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.1se
lasso_coefficients = coef(cv_lasso, s = "lambda.1se")

coef_df <- as.data.frame(as.matrix(lasso_coefficients))
coef_df$Question <- rownames(coef_df)
names(coef_df)[1] <- "Coefficient"

selected_vars <- coef_df[coef_df$Coefficient != 0, ]
selected_vars
```

### Quick Check on Life Satisfaction

```{r}
table(wvs_clean$Q49)
hist(wvs_clean$Q49)
```

### Fitting Models

```{r}
# May not ever need intercept, consider removing it from selected_vars entirely
selected_var_names <- selected_vars$Question[selected_vars$Question != "(Intercept)"]

# create training and test dataframes
train_data <- data.frame(
  Q49 = y_train,
  X_train[, selected_var_names]
)

test_data <- data.frame(
  Q49 = y_test,
  X_test[, selected_var_names]
)

# fit model 1: Linear Regression
lm <- lm(Q49 ~ ., data = train_data)

# fit model 2: Proportional Odds
po <- polr(factor(Q49) ~ ., data = train_data)

#summarize optional, output is very long
```

### Diagnostics

```{r}
# Linear model diagnostics
par(mfrow = c(2, 2))
plot(lm)
par(mfrow = c(1, 1))
```

### Evaluate on Test Set

```{r}
# predictions
pred_orig_lm <- predict(original_lm, newdata = as.data.frame(X_test[, original_predictors]))
pred_lm <- predict(lm, newdata = test_data)
pred_po <- as.numeric(as.character(predict(po, newdata = test_data)))

# metrics
mae_orig_lm <- mean(abs(pred_orig_lm - test_data$Q49))
mae_lm <- mean(abs(pred_lm - test_data$Q49))
mae_po <- mean(abs(pred_po - test_data$Q49))

rmse_orig_lm <- sqrt(mean((pred_orig_lm - test_data$Q49)^2))
rmse_lm <- sqrt(mean((pred_lm - test_data$Q49)^2))
rmse_po <- sqrt(mean((pred_po - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Original Linear (manual variable selection)", "Linear (LASSO)", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_orig_lm, mae_lm, mae_po),
  Root_Mean_Square_Error = c(rmse_orig_lm, rmse_lm, rmse_po)
)
```

Hunch that the intermediate values from lm is giving it better metrics than the proportional odds model which is forced to round to an integer.

### Evaluate on Test Set (pt 2)

```{r}
# try using expected value (continuous) instead of mode
pred_po_probs <- predict(po, newdata = test_data, type = "probs")
pred_po_expected <- as.numeric(pred_po_probs %*% 1:10)

mae_po_expected <- mean(abs(pred_po_expected - test_data$Q49))
rmse_po_expected <- sqrt(mean((pred_po_expected - test_data$Q49)^2))

# display comparison
data.frame(
  Model = c("Original Linear (manual variable selection)", "Linear (LASSO)", "Proportional Odds"),
  Mean_Absolute_Error = c(mae_orig_lm, mae_lm, mae_po_expected),
  Root_Mean_Square_Error = c(rmse_orig_lm, rmse_lm, rmse_po_expected)
)
```

These results indicate that the original linear model (manual variable selection) is a worse fit compared to the other two, which are still very similar. This is better but still bad, let's check proportional odds assumptions with brant test. Warning: this takes forever to run.

```{r}
library(brant)
brant(po)
```

Findings: assumptions hold for neither model

### 10-Fold CV

```{r}
k <- 10
folds <- sample(1:k, nrow(train_data), replace = TRUE)

cv_errors_lm <- numeric(k)
cv_errors_po <- numeric(k)

for(i in 1:k) {
  # split
  cv_train <- train_data[folds != i, ]
  cv_valid <- train_data[folds == i, ]
  
  # Fit
  fit_lm <- lm(Q49 ~ ., data = cv_train)
  fit_po <- polr(factor(Q49) ~ ., data = cv_train)
  
  # predict
  pred_lm <- predict(fit_lm, newdata = cv_valid)
  pred_po_probs <- predict(fit_po, newdata = cv_valid, type = "probs")
  pred_po <- as.numeric(pred_po_probs %*% 1:10)
  
  # calculate errors
  cv_errors_lm[i] <- mean((pred_lm - cv_valid$Q49)^2)
  cv_errors_po[i] <- mean((pred_po - cv_valid$Q49)^2)
}

# Results
cv_rmse_lm <- sqrt(mean(cv_errors_lm))
cv_rmse_po <- sqrt(mean(cv_errors_po))

# Compare
data.frame(
  Model = c("Linear", "Proportional Odds"),
  Test_RMSE = c(1.642, 1.649),
  CV_RMSE = c(cv_rmse_lm, cv_rmse_po)
)
```

The output follows the above trends. We choose the **Linear Regression** model as it's the cheaper of the two models.

### Sensitivity Analyses
#### E and F (Privacy and Interest)
```{r}
# Clean wvs
sensitivity_vars <- c("Q49", selected_var_names, "E_RESPINT", "F_INTPRIVACY")

wvs_sensitivity <- wvs[, sensitivity_vars]

wvs_sensitivity_clean <- na.omit(wvs_sensitivity)

# Check what E and F look like
cat("E_RESPINT (Respondent Interest) distribution:\n")
table(wvs_sensitivity_clean$E_RESPINT)

cat("\nF_INTPRIVACY (Interview Privacy) distribution:\n")
table(wvs_sensitivity_clean$F_INTPRIVACY)
```
Let's say E_RESPINT = 1,2 and F_INTPRIVACY = 1 are high quality values.

```{r}
# create train and test split
n <- nrow(wvs_sensitivity_clean)
train_indices_sens <- sample(1:n, size = round(0.7 * n))

train_sens <- wvs_sensitivity_clean[train_indices_sens, ]
test_sens <- wvs_sensitivity_clean[-train_indices_sens, ]

# filter to high quality values only
train_high <- train_sens[train_sens$E_RESPINT >= 0 & 
                         train_sens$E_RESPINT <= 2 &
                         train_sens$F_INTPRIVACY == 1, ]

test_high <- test_sens[test_sens$E_RESPINT >= 0 & 
                       test_sens$E_RESPINT <= 2 &
                       test_sens$F_INTPRIVACY == 1, ]

# Remove E and F from predictors
predictor_vars <- setdiff(names(train_sens), c("Q49", "E_RESPINT", "F_INTPRIVACY"))

# fit models
lm_full <- lm(Q49 ~ ., data = train_sens[, c("Q49", predictor_vars)])

lm_high <- lm(Q49 ~ ., data = train_high[, c("Q49", predictor_vars)])

# Predict
pred_full <- predict(lm_full, newdata = test_sens)
mae_full <- mean(abs(pred_full - test_sens$Q49))
rmse_full <- sqrt(mean((pred_full - test_sens$Q49)^2))

pred_high <- predict(lm_high, newdata = test_high)
mae_high <- mean(abs(pred_high - test_high$Q49))
rmse_high <- sqrt(mean((pred_high - test_high$Q49)^2))

# display
data.frame(
  Dataset = c("All Responses", "High Quality Only"),
  N_train = c(nrow(train_sens), nrow(train_high)),
  N_test = c(nrow(test_sens), nrow(test_high)),
  MAE = c(mae_full, mae_high),
  RMSE = c(rmse_full, rmse_high)
)
```
Slight improvement with "high-quality" data, but the inclusion of lower-quality responses does not substantially degrade the model accuracy. This supports the validity of using the full dataset for final predictions.

#### General Robustness
```{r}
# conduct sensitivity analyses / check robustness of results

library(glmnet)
library(dplyr)

rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))
mae  <- function(y, yhat) mean(abs(y - yhat))

set.seed(123)
n <- nrow(X)
train_indices <- sample(1:n, size = round(0.7 * n))
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test  <- X[-train_indices, ]
y_test  <- y[-train_indices]

cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)

pred_1se <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.1se"))
pred_min <- as.numeric(predict(cv_lasso, newx = X_test, s = "lambda.min"))

lambda_perf <- data.frame(
  lambda_rule = c("lambda.1se", "lambda.min"),
  RMSE = c(rmse(y_test, pred_1se), rmse(y_test, pred_min)),
  MAE  = c(mae(y_test, pred_1se),  mae(y_test, pred_min))
)

coef_1se <- coef(cv_lasso, s = "lambda.1se")
coef_min <- coef(cv_lasso, s = "lambda.min")

vars_1se <- setdiff(rownames(coef_1se)[as.vector(coef_1se != 0)], "(Intercept)")
vars_min <- setdiff(rownames(coef_min)[as.vector(coef_min != 0)], "(Intercept)")

jaccard_overlap <- length(intersect(vars_1se, vars_min)) / length(union(vars_1se, vars_min))

print(lambda_perf)
length(vars_1se)
length(vars_min)
jaccard_overlap

seeds <- 1:30
split_df <- bind_rows(lapply(seeds, function(s) {
  set.seed(s)
  n <- nrow(X)
  tr <- sample(1:n, size = round(0.7 * n))
  X_tr <- X[tr, ]; y_tr <- y[tr]
  X_te <- X[-tr, ]; y_te <- y[-tr]

  cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
  yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
  nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1

  data.frame(
    seed = s,
    RMSE = rmse(y_te, yhat),
    MAE  = mae(y_te, yhat),
    n_selected = nsel
  )
}))

split_df %>%
  summarize(
    RMSE_mean = mean(RMSE), RMSE_sd = sd(RMSE),
    MAE_mean  = mean(MAE),  MAE_sd  = sd(MAE),
    nsel_mean = mean(n_selected), nsel_sd = sd(n_selected)
  )

sensitive_vars <- c("Q260","Q262","Q273","Q240","Q150")
sens_in_data <- intersect(sensitive_vars, colnames(X))

run_lasso_once <- function(Xmat, yvec, seed = 123) {
  set.seed(seed)
  n <- nrow(Xmat)
  tr <- sample(1:n, size = round(0.7 * n))
  X_tr <- Xmat[tr, ]; y_tr <- yvec[tr]
  X_te <- Xmat[-tr, ]; y_te <- yvec[-tr]

  cv_fit <- cv.glmnet(X_tr, y_tr, alpha = 1)
  yhat <- as.numeric(predict(cv_fit, newx = X_te, s = "lambda.1se"))
  nsel <- sum(coef(cv_fit, s = "lambda.1se") != 0) - 1

  data.frame(
    RMSE = rmse(y_te, yhat),
    MAE  = mae(y_te, yhat),
    n_selected = nsel
  )
}

full_perf <- run_lasso_once(X, y)

if (length(sens_in_data) > 0) {
  X_nosens <- X[, setdiff(colnames(X), sens_in_data), drop = FALSE]
  nosens_perf <- run_lasso_once(X_nosens, y)
  rbind(
    data.frame(model = "All variables", full_perf),
    data.frame(model = "Exclude sensitive vars", nosens_perf)
  )
} else {
  data.frame(model = "All variables", full_perf)
}
```
